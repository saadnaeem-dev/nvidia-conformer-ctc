{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e2d416",
   "metadata": {},
   "source": [
    "### IMPORTING RIVA NeMo LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa638b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[NeMo W 2023-04-28 20:05:52 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-04-28 20:05:53 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.core.classes import IterableDataset\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49b80",
   "metadata": {},
   "source": [
    "### LOAD NeMO MODEL --- CONFORMER-CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f40936",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:05.288834Z",
     "end_time": "2023-04-28T20:06:08.825948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-28 20:06:07 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-28 20:06:08 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2023-04-28 20:06:08 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2023-04-28 20:06:08 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-28 20:06:08 features:287] PADDING: 0\n",
      "[NeMo I 2023-04-28 20:06:08 save_restore_connector:247] Model EncDecCTCModelBPE was successfully restored from C:\\Users\\saadn\\.cache\\huggingface\\hub\\models--nvidia--stt_en_conformer_ctc_large\\snapshots\\2c8326e4e43ae5b994612cfea3f3029818fb23c6\\stt_en_conformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(\"nvidia/stt_en_conformer_ctc_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a649bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:23.391272Z",
     "end_time": "2023-04-28T20:06:23.405272Z"
    }
   },
   "outputs": [],
   "source": [
    "def getRivaTranscript(audio) -> str:\n",
    "    # Convert audio tensor to a numpy array\n",
    "    samples = audio.get_array_of_samples()\n",
    "    audio = np.array(samples)\n",
    "    \n",
    "    # Set up buffer and chunk lengths, stride, and sample rate\n",
    "    context_len_in_secs = 1\n",
    "    chunk_len_in_secs = 15            \n",
    "    stride = 4\n",
    "    sample_rate = 16000\n",
    "    chunk_len = sample_rate * chunk_len_in_secs\n",
    "    buffer_len_in_secs = chunk_len_in_secs + 2 * context_len_in_secs\n",
    "    buffer_len = sample_rate * buffer_len_in_secs\n",
    "    sampbuffer = np.zeros([buffer_len], dtype=np.float32)\n",
    "    \n",
    "    # Read chunks of audio\n",
    "    chunk_reader = AudioChunkIterator(audio, chunk_len_in_secs, sample_rate)\n",
    "    buffer_list = []\n",
    "    for chunk in chunk_reader:\n",
    "        sampbuffer[:-chunk_len] = sampbuffer[chunk_len:]\n",
    "        sampbuffer[-chunk_len:] = chunk\n",
    "        buffer_list.append(np.array(sampbuffer))\n",
    "    \n",
    "    # Transcribe the audio chunks\n",
    "    asr_decoder = ChunkBufferDecoder(model, stride=stride, chunk_len_in_secs=chunk_len_in_secs, buffer_len_in_secs=buffer_len_in_secs)\n",
    "    transcription = asr_decoder.transcribe_buffers(buffer_list, plot=False)    \n",
    "    \n",
    "    # Return the transcription\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab802b00",
   "metadata": {},
   "source": [
    "## Support Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800b63f3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:31.107540Z",
     "end_time": "2023-04-28T20:06:31.184410Z"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from IPython.display import Audio\n",
    "from numpy.fft import fft, ifft\n",
    "from pydub import AudioSegment\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad39a4d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:34.347019Z",
     "end_time": "2023-04-28T20:06:34.371412Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioChunkIterator():\n",
    "    def __init__(self, samples, frame_len, sample_rate):\n",
    "        self._samples = samples\n",
    "        self._chunk_len = frame_len*sample_rate\n",
    "        self._start = 0\n",
    "        self.output=True\n",
    "   \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        last = int(self._start + self._chunk_len)\n",
    "        if last <= len(self._samples):\n",
    "            chunk = self._samples[self._start: last]\n",
    "            self._start = last\n",
    "        else:\n",
    "            chunk = np.zeros([int(self._chunk_len)], dtype='float32')\n",
    "            samp_len = len(self._samples) - self._start\n",
    "            chunk[0:samp_len] = self._samples[self._start:len(self._samples)]\n",
    "            self.output = False\n",
    "        return chunk\n",
    "\n",
    "def speech_collate_fn(batch):\n",
    "    \"\"\"collate batch of audio sig, audio len\n",
    "    Args:\n",
    "        batch (FloatTensor, LongTensor):  A tuple of tuples of signal, signal lengths.\n",
    "        This collate func assumes the signals are 1d torch tensors (i.e. mono audio).\n",
    "    \"\"\"\n",
    "    _, audio_lengths = zip(*batch)\n",
    "\n",
    "    max_audio_len = 0\n",
    "    has_audio = audio_lengths[0] is not None\n",
    "    if has_audio:\n",
    "        max_audio_len = max(audio_lengths).item()\n",
    "   \n",
    "    audio_signal= []\n",
    "    for sig, sig_len in batch:\n",
    "        if has_audio:\n",
    "            sig_len = sig_len.item()\n",
    "            if sig_len < max_audio_len:\n",
    "                pad = (0, max_audio_len - sig_len)\n",
    "                sig = torch.nn.functional.pad(sig, pad)\n",
    "            audio_signal.append(sig)\n",
    "        \n",
    "    if has_audio:\n",
    "        audio_signal = torch.stack(audio_signal)\n",
    "        audio_lengths = torch.stack(audio_lengths)\n",
    "    else:\n",
    "        audio_signal, audio_lengths = None, None\n",
    "\n",
    "    return audio_signal, audio_lengths\n",
    "\n",
    "# simple data layer to pass audio signal\n",
    "class AudioBuffersDataLayer(IterableDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._buf_count == len(self.signal) :\n",
    "            raise StopIteration\n",
    "        self._buf_count +=1\n",
    "        return torch.as_tensor(self.signal[self._buf_count-1], dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape[0], dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signals):\n",
    "        self.signal = signals\n",
    "        self.signal_shape = self.signal[0].shape\n",
    "        self._buf_count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "class ChunkBufferDecoder:\n",
    "    def __init__(self,asr_model, stride, chunk_len_in_secs=1, buffer_len_in_secs=3):\n",
    "        self.asr_model = asr_model\n",
    "        self.asr_model.eval()\n",
    "        self.data_layer = AudioBuffersDataLayer()\n",
    "        self.data_loader = DataLoader(self.data_layer, batch_size=1, collate_fn=speech_collate_fn)\n",
    "        self.buffers = []\n",
    "        self.all_preds = []\n",
    "        self.chunk_len = chunk_len_in_secs\n",
    "        self.buffer_len = buffer_len_in_secs\n",
    "        assert(chunk_len_in_secs<=buffer_len_in_secs)\n",
    "        \n",
    "        feature_stride = asr_model._cfg.preprocessor['window_stride']\n",
    "        self.model_stride_in_secs = feature_stride * stride\n",
    "        self.n_tokens_per_chunk = math.ceil(self.chunk_len / self.model_stride_in_secs)\n",
    "        self.blank_id = len(asr_model.decoder.vocabulary)\n",
    "        self.plot=False\n",
    "        \n",
    "    @torch.no_grad()    \n",
    "    def transcribe_buffers(self, buffers, merge=True, plot=False):\n",
    "        self.plot = plot\n",
    "        self.buffers = buffers\n",
    "        self.data_layer.set_signal(buffers[:])\n",
    "        self._get_batch_preds()      \n",
    "        return self.decode_final(merge)\n",
    "    \n",
    "    def _get_batch_preds(self):\n",
    "        device = self.asr_model.device\n",
    "        for batch in iter(self.data_loader):\n",
    "\n",
    "            audio_signal, audio_signal_len = batch\n",
    "\n",
    "            audio_signal, audio_signal_len = audio_signal.to(device), audio_signal_len.to(device)\n",
    "            log_probs, encoded_len, predictions = self.asr_model(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
    "            preds = torch.unbind(predictions)\n",
    "            for pred in preds:\n",
    "                self.all_preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    def decode_final(self, merge=True, extra=0):\n",
    "        self.unmerged = []\n",
    "        self.toks_unmerged = []\n",
    "        # index for the first token corresponding to a chunk of audio would be len(decoded) - 1 - delay\n",
    "        delay = math.ceil((self.chunk_len + (self.buffer_len - self.chunk_len) / 2) / self.model_stride_in_secs)\n",
    "\n",
    "        decoded_frames = []\n",
    "        all_toks = []\n",
    "        for pred in self.all_preds:\n",
    "            ids, toks = self._greedy_decoder(pred, self.asr_model.tokenizer)\n",
    "            decoded_frames.append(ids)\n",
    "            all_toks.append(toks)\n",
    "\n",
    "        for decoded in decoded_frames:\n",
    "            self.unmerged += decoded[len(decoded) - 1 - delay:len(decoded) - 1 - delay + self.n_tokens_per_chunk]\n",
    "        if self.plot:\n",
    "            for i, tok in enumerate(all_toks):\n",
    "                plt.plot(self.buffers[i])\n",
    "                plt.show()\n",
    "                print(\"\\nGreedy labels collected from this buffer\")\n",
    "                print(tok[len(tok) - 1 - delay:len(tok) - 1 - delay + self.n_tokens_per_chunk])                \n",
    "                self.toks_unmerged += tok[len(tok) - 1 - delay:len(tok) - 1 - delay + self.n_tokens_per_chunk]\n",
    "            print(\"\\nTokens collected from succesive buffers before CTC merge\")\n",
    "            print(self.toks_unmerged)\n",
    "\n",
    "        if not merge:\n",
    "            return self.unmerged\n",
    "        return self.greedy_merge(self.unmerged)\n",
    "    \n",
    "    def _greedy_decoder(self, preds, tokenizer):\n",
    "        s = []\n",
    "        ids = []\n",
    "        for i in range(preds.shape[0]):\n",
    "            if preds[i] == self.blank_id:\n",
    "                s.append(\"_\")\n",
    "            else:\n",
    "                pred = preds[i]\n",
    "                s.append(tokenizer.ids_to_tokens([pred.item()])[0])\n",
    "            ids.append(preds[i])\n",
    "        return ids, s\n",
    "         \n",
    "    def greedy_merge(self, preds):\n",
    "        decoded_prediction = []\n",
    "        previous = self.blank_id\n",
    "        for p in preds:\n",
    "            if (p != previous or previous == self.blank_id) and p != self.blank_id:\n",
    "                decoded_prediction.append(p.item())\n",
    "            previous = p\n",
    "        hypothesis = self.asr_model.tokenizer.ids_to_text(decoded_prediction)\n",
    "        return hypothesis\n",
    "    \n",
    "def plot_audio(audio):\n",
    "    samples = audio.get_array_of_samples()\n",
    "    np.array(samples)\n",
    "    plt.figure(figsize=[14,5])\n",
    "    plt.plot(samples)\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Waveform of Agent Audio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6abe43e0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:36.301586Z",
     "end_time": "2023-04-28T20:06:36.308595Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_conversation(list1, list2, column=3):\n",
    "    merged_list = []\n",
    "    i = j = 0\n",
    "    n1, n2 = len(list1), len(list2)\n",
    "    while i < n1 and j < n2:\n",
    "        if list1[i][column] <= list2[j][column]:\n",
    "            merged_list.append(list1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged_list.append(list2[j])\n",
    "            j += 1\n",
    "    merged_list.extend(list1[i:])\n",
    "    merged_list.extend(list2[j:])\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd17e61",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:37.467578Z",
     "end_time": "2023-04-28T20:06:37.482611Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import silence\n",
    "\n",
    "def split_on_silence(audio, silence_detect_fn, label):\n",
    "    silences = silence_detect_fn(audio)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i, (start_silence, end_silence) in enumerate(silences):\n",
    "        transcript = \"\"\n",
    "        end = start_silence * 1000\n",
    "        mid =  (start + end) / 2\n",
    "        chunk = audio[start:end]\n",
    "        if chunk.duration_seconds > 0.5:\n",
    "            transcript = getRivaTranscript(chunk)       ## TRANSCRIPTION OF CHUNK THROIUGH RIVA   \n",
    "        chunks.append((label, start/1000, end/1000, mid, chunk, transcript))\n",
    "        start = end_silence * 1000\n",
    "    end = audio.duration_seconds * 1000\n",
    "    mid =  (start + end) / 2\n",
    "    chunk = audio[start:end]\n",
    "    if chunk.duration_seconds > 0.5:\n",
    "        transcript = getRivaTranscript(chunk)       ## TRANSCRIPTION OF CHUNK THROIUGH RIVA   \n",
    "    chunks.append((label, start/1000, end/1000, mid, chunk, transcript))\n",
    "    return chunks\n",
    "\n",
    "def extract_silences(audio, dBFS=None):\n",
    "    if dBFS is None:\n",
    "        dBFS = audio.dBFS\n",
    "    sil = silence.detect_silence(audio, min_silence_len=1000, silence_thresh=dBFS-16)\n",
    "    return [(start/1000, stop/1000) for start, stop in sil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33499a96",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:06:38.550718Z",
     "end_time": "2023-04-28T20:06:38.569715Z"
    }
   },
   "outputs": [],
   "source": [
    "def dialog_transcription(agent_file, customer_file):\n",
    "    # Read Audio files using pydub\n",
    "    agent_audio = AudioSegment.from_file(agent_file)\n",
    "    customer_audio = AudioSegment.from_file(customer_file)\n",
    "        \n",
    "    #Resampling to 16k\n",
    "    agent_audio = agent_audio.set_frame_rate(16000)\n",
    "    customer_audio = customer_audio.set_frame_rate(16000)\n",
    "    \n",
    "    #split audios based on silences then merge and order\n",
    "    agent_chunks = split_on_silence(agent_audio, extract_silences, \"Agent\")\n",
    "    customer_chunks = split_on_silence(customer_audio, extract_silences, \"Customer\")\n",
    "    dialog =  merge_conversation(agent_chunks, customer_chunks)\n",
    "    \n",
    "    # Concatenate transcript of the chunks with their timestamps\n",
    "    result = \"\"\n",
    "    for label, start, end, mid, chunk, transcript in dialog:\n",
    "        if chunk.duration_seconds > 0.5: \n",
    "            #print(\"[{:<4.2f}, {:<4.2f}] {:<8}: {}\".format(start, end, label, transcript))\n",
    "            chunk_str = \"[{:<4.2f}, {:<4.2f}] {:<8} :{}\\n\".format(start, end, label, transcript)\n",
    "            result += chunk_str\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d3ce7",
   "metadata": {},
   "source": [
    "## TEST TRANSCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691200e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:07:28.066357Z",
     "end_time": "2023-04-28T20:07:52.523349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00, 3.04] Agent    :ternoon this is sou is speaking how may i help you\n",
      "[7.67, 13.28] Customer :helloso my name is mitin yanda and i'm calling because we have issues with our internet connect\n",
      "[11.82, 14.42] Agent    :liir could you please provide me with a bortai\n",
      "[19.08, 26.74] Customer :during the day or internet is fad but during the night the connection is very bad could cause that\n",
      "[26.34, 29.73] Agent    :this is because during the night there is a lot of more traffic\n",
      "[34.94, 36.51] Customer :what we should doy\n",
      "[36.09, 40.18] Agent    :my subpicion is to send you a new router that will be more powerful\n",
      "[44.70, 45.45] Customer :the suny\n",
      "[44.57, 49.20] Agent    :i will inform o technical team to visit you and implement a new route\n",
      "[51.96, 55.75] Agent    :could you please provide me with your address mr yano\n",
      "[60.57, 62.41] Customer :brs tuding stn\n",
      "[61.68, 65.45] Agent    :thank you wou next tuesday at three pm woul you\n",
      "[69.88, 72.59] Customer :yes fine i will be at home at thattime\n",
      "[71.71, 72.94] Agent    :haveank you called s\n",
      "[76.83, 77.41] Agent    :ho i sy\n",
      "[77.80, 79.02] Customer :thank you for your time sir\n",
      "[81.71, 82.31] Customer :\n",
      "[89.92, 92.24] Agent    :then you shoud turn the callse\n",
      "[96.38, 97.00] Customer :ey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transcript = dialog_transcription(\"MegaAudio/utr7lk3r1qfnjhcp41f7_1_TFS_600004.wav\", \"MegaAudio/utr7lk3r1qfnjhcp41f7_600002.wav\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e85a6441",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:07:59.368811Z",
     "end_time": "2023-04-28T20:08:14.704844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.80, 6.66] Agent    :good morning thank you for calling dr nron salon this is matt speaking how may i assist you today\n",
      "[7.78, 11.82] Customer :hy i was wondering if i could book an appointment for a hair cut\n",
      "[13.55, 18.81] Agent    :of course i'ld be happy to help you with that can you please tell me the date and the time that worked for you\n",
      "[20.23, 25.81] Customer :i was thinking about next thursday at two pm will that be available\n",
      "[27.62, 29.24] Agent    :uh let me check our schedule for you\n",
      "[30.74, 34.53] Agent    :yes we have an opening for an haircut at two pm next thursday\n",
      "[35.62, 37.83] Customer :mat can you please look up for me\n",
      "[39.78, 41.51] Agent    :certainly may i have your name plea\n",
      "[42.35, 44.06] Customer :my name is jennifer\n",
      "[45.67, 53.60] Agent    :perfect jennifer i have booked you a haircut at two p m next thursday may i have your phone number in case we need to reach you\n",
      "[54.79, 60.84] Customer :sure my phone number is four one five five one three one one three nine\n",
      "[63.59, 71.31] Agent    :thank you i have your appointment confirmed and your contact information on file is there anything else i can help you out with today\n",
      "[72.36, 73.78] Customer :no not all thank you\n",
      "[75.79, 79.02] Agent    :re welcome jennifer we look forward to seeing you next thursday have a great day\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transcript = dialog_transcription(\"MegaAudio/d3nn9h1jsqje8dv9v0pq_1_TFS_400059.wav\", \"MegaAudio/d3nn9h1jsqje8dv9v0pq_101011.wav\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58260c4c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:08:18.884710Z",
     "end_time": "2023-04-28T20:08:40.493290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.80, 5.72] Agent    :hello this is sakan from affinity insurance how are you doing today\n",
      "[7.80, 9.24] Customer :hello i'm good thank you\n",
      "[10.90, 20.05] Agent    :i'm calling because i notice that you may be in need of insurance coverage for your car can i ask you a few questions to determine the best coverage options for you\n",
      "[22.26, 23.11] Customer :nine\n",
      "[24.38, 28.98] Agent    :waight let me start by asking do you currently have insurance coverage for your car\n",
      "[30.78, 32.90] Customer :yes i have insurance coverage for my c\n",
      "[33.82, 39.84] Agent    :a that's great can i ask what type of coverage you have and who is your current provider\n",
      "[41.98, 46.58] Customer :i have to covered with hundred percent payment premiums with aliiance insurance\n",
      "[47.66, 56.53] Agent    :okay thank you let me take a moment to review your current coverage and see if there are any areas where we can improve your coverage or offer a better one\n",
      "[58.77, 59.63] Customer :okay s\n",
      "[62.70, 79.72] Agent    :great i have reviewed your current coverage and it looks like we can offer you lower premium rates and better discounts at the same time with our insurance coverage you'll have access to twenty four seven customer support as well as a rangge of discounts for things like politicpal policies and safety\n",
      "[82.20, 86.33] Customer :that sounds interesting can you give me a qute for the covert you are offer\n",
      "[87.46, 92.49] Agent    :absolutely can you please provide me some information so that i can give you an accurate code\n",
      "[95.08, 99.29] Customer :suw i am sure to pay four hundred dollars a month\n",
      "[100.31, 108.13] Agent    :soh that's great based on the information you have provided i can offer you a coverage for just two hundred and fifty dollars a month\n",
      "[110.85, 114.50] Customer :that sounds good can i think of on it them g pat you\n",
      "[115.47, 130.19] Agent    :of course i can understand that buying insurance is a big decision take your time and let me know if you have any questions here is my direct line four one five five one three one one three nine i look forw\n",
      "[132.52, 137.97] Customer :oka thank you for your time i will definitely consider your refert have a great day\n",
      "[139.12, 142.88] Agent    :you are welcome thank you for considering authility insurance have a great day as well\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transcript = dialog_transcription(\"MegaAudio/1snipln0j6obdfsbeqhv_400059.wav\", \"MegaAudio/1snipln0j6obdfsbeqhv_1_TFS_101011.wav\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f57a3ff7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T20:29:51.222411Z",
     "end_time": "2023-04-28T20:29:51.238391Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\saadn\\AppData\\Local\\Temp\\ipykernel_8484\\3650518360.py\u001B[0m:\u001B[94m1\u001B[0m in \u001B[92m<module>\u001B[0m                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: \u001B[0m                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[3;31m'C:\\\\Users\\\\saadn\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8484\\\\3650518360.py'\u001B[0m                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torch\\cuda\\__init__.py\u001B[0m: \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[94m341\u001B[0m in \u001B[92mget_device_name\u001B[0m                                                                           \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 338 \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mset_device\u001B[0m(device: _device_t) -> \u001B[94mNone\u001B[0m:                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 339 \u001B[0m\u001B[2;90m│   \u001B[0m\u001B[33mr\u001B[0m\u001B[33m\"\"\"Sets the current device.\u001B[0m                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 340 \u001B[0m\u001B[2m│   \u001B[0m                                                                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 341 \u001B[2;33m│   \u001B[0m\u001B[33mUsage of this function is discouraged in favor of :any:`device`. In most\u001B[0m              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 342 \u001B[0m\u001B[2;33m│   \u001B[0m\u001B[33mcases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.\u001B[0m             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 343 \u001B[0m\u001B[2m│   \u001B[0m                                                                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 344 \u001B[0m\u001B[2;33m│   \u001B[0m\u001B[33mArgs:\u001B[0m                                                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torch\\cuda\\__init__.py\u001B[0m: \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[94m371\u001B[0m in \u001B[92mget_device_properties\u001B[0m                                                                     \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 368 \u001B[0m\u001B[94mdef\u001B[0m \u001B[92mget_device_capability\u001B[0m(device: Optional[_device_t] = \u001B[94mNone\u001B[0m) -> Tuple[\u001B[96mint\u001B[0m, \u001B[96mint\u001B[0m]:         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 369 \u001B[0m\u001B[2;90m│   \u001B[0m\u001B[33mr\u001B[0m\u001B[33m\"\"\"Gets the cuda capability of a device.\u001B[0m                                             \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 370 \u001B[0m\u001B[2m│   \u001B[0m                                                                                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 371 \u001B[2;33m│   \u001B[0m\u001B[33mArgs:\u001B[0m                                                                                 \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 372 \u001B[0m\u001B[2;33m│   │   \u001B[0m\u001B[33mdevice (torch.device or int, optional): device for which to return the\u001B[0m            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 373 \u001B[0m\u001B[2;33m│   │   │   \u001B[0m\u001B[33mdevice capability. This function is a no-op if this argument is\u001B[0m               \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 374 \u001B[0m\u001B[2;33m│   │   │   \u001B[0m\u001B[33ma negative integer. It uses the current device, given by\u001B[0m                      \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[33mC:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torch\\cuda\\__init__.py\u001B[0m: \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[94m221\u001B[0m in \u001B[92m_lazy_init\u001B[0m                                                                                \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 218 \u001B[0m                                                                                          \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 219 \u001B[0m\u001B[94mdef\u001B[0m \u001B[92m_lazy_init\u001B[0m():                                                                         \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 220 \u001B[0m\u001B[2m│   \u001B[0m\u001B[94mglobal\u001B[0m _initialized, _queued_calls                                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 221 \u001B[2m│   \u001B[0m\u001B[94mif\u001B[0m is_initialized() \u001B[95mor\u001B[0m \u001B[96mhasattr\u001B[0m(_tls, \u001B[33m'\u001B[0m\u001B[33mis_initializing\u001B[0m\u001B[33m'\u001B[0m):                              \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 222 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m                                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 223 \u001B[0m\u001B[2m│   \u001B[0m\u001B[94mwith\u001B[0m _initialization_lock:                                                            \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m 224 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[2m# We be double-checked locking, boys!  This is OK because\u001B[0m                         \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mAssertionError: \u001B[0mTorch not compiled with CUDA enabled\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\saadn\\AppData\\Local\\Temp\\ipykernel_8484\\3650518360.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\saadn\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8484\\\\3650518360.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torch\\cuda\\__init__.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">341</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_device_name</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 338 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">set_device</span>(device: _device_t) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 339 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Sets the current device.</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 340 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 341 <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">Usage of this function is discouraged in favor of :any:`device`. In most</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 342 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 343 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 344 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">Args:</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torch\\cuda\\__init__.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">371</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_device_properties</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 368 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_device_capability</span>(device: Optional[_device_t] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) -&gt; Tuple[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>]:         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 369 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Gets the cuda capability of a device.</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 370 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 371 <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">Args:</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 372 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">device (torch.device or int, optional): device for which to return the</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 373 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">device capability. This function is a no-op if this argument is</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 374 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">a negative integer. It uses the current device, given by</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\saadn\\anaconda3\\envs\\nvidia-riva-hugging-face\\lib\\site-packages\\torch\\cuda\\__init__.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">221</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_init</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 218 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 219 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_init</span>():                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 220 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">global</span> _initialized, _queued_calls                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 221 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_initialized() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(_tls, <span style=\"color: #808000; text-decoration-color: #808000\">'is_initializing'</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 222 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 223 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> _initialization_lock:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 224 │   │   # We be double-checked locking, boys!  This is OK because</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>Torch not compiled with CUDA enabled\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
